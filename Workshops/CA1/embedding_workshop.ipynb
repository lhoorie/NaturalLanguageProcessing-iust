{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCb3cVwzrfbb",
        "outputId": "6f9ff9de-9124-498a-bd81-f45661aa039f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_DIR = '/content/gdrive/MyDrive/mahsa_amini_data.zip'"
      ],
      "metadata": {
        "id": "SXT4_E5qePVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/gdrive/MyDrive/mahsa_amini_data.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHQqTYO5e671",
        "outputId": "14a2c992-2850-49d6-8183-ec33eabf7ad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/gdrive/MyDrive/mahsa_amini_data.zip\n",
            "replace mahsa_amini_data.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: mahsa_amini_data.csv    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/mahsa_amini_data.csv'"
      ],
      "metadata": {
        "id": "Z4_5Td-QfNHm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q"
      ],
      "metadata": {
        "id": "ghSKa_EcR693"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "6iSmmQgAipNm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from collections import OrderedDict\n",
        "from datasets import load_dataset\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "HtCgCrUVtU_J"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)"
      ],
      "metadata": {
        "id": "pfDri3MGtFm-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_k_nearest_neighbors(word, embedding_dict, k):\n",
        "  words_cosine_similarity = dict()\n",
        "  for token in embedding_dict.keys():\n",
        "    words_cosine_similarity[token] = cos(embedding_dict[word], embedding_dict[token]).item()\n",
        "  words_cosine_similarity = dict(sorted(words_cosine_similarity.items(), key=lambda item: item[1]))\n",
        "  return list(words_cosine_similarity.keys())[-k:][::-1]\n",
        "\n",
        "def delete_hashtag_usernames(text):\n",
        "  try:\n",
        "    result = []\n",
        "    for word in text.split():\n",
        "      if word[0] not in ['@', '#']:\n",
        "        result.append(word)\n",
        "    return ' '.join(result)\n",
        "  except:\n",
        "    return ''\n",
        "\n",
        "def delete_url(text):\n",
        "  text = re.sub(r'http\\S+', '', text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "UXy2OcR7s84V"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"Ø²Ù†Ø¯Ú¯ÛŒ\"\n",
        "k = 10"
      ],
      "metadata": {
        "id": "mf-3eSz6udw1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0. Data preprocessing"
      ],
      "metadata": {
        "id": "cz4cWWgA4xBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. extract all tweets from files and save them in memory base on each year.\n",
        "# 2. remove urls, hashtags and usernames.\n",
        "corpus = []\n",
        "my_file = pd.read_csv(path)['Text']"
      ],
      "metadata": {
        "id": "bG5awXKo40HS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in my_file:\n",
        "  temp = delete_hashtag_usernames(line)\n",
        "  corpus.append(delete_url(temp))"
      ],
      "metadata": {
        "id": "nngkD00LjE3i"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. One hot encoding"
      ],
      "metadata": {
        "id": "Nqr5DLDYuKd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. find one hot encoding of each word for each year\n",
        "# 2. find 10 nearest words from \"Ø²Ù†Ø¯Ú¯ÛŒ\"\n",
        "distinct_words = set()\n",
        "for line in corpus:\n",
        "  for w in line.split(' '):\n",
        "    distinct_words.add(w)\n",
        "\n",
        "l = len(distinct_words)\n",
        "embedding_dict = {}\n",
        "for i, w in enumerate(distinct_words):\n",
        "  v1 = np.zeros(l)\n",
        "  v1[i] = 1\n",
        "  v2 = torch.from_numpy(v1)\n",
        "  embedding_dict[w] = v2\n",
        "\n",
        "neighbors = find_k_nearest_neighbors(word, embedding_dict, k)\n",
        "neighbors"
      ],
      "metadata": {
        "id": "OPqc0I0yuNlI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9be30cff-61a9-429c-92d7-4d0b1688d432"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ø¢Ø²Ø§Ø¯ÛŒ',\n",
              " 'Ø¯Ø§Ø¯ÛŒÙ†.',\n",
              " 'ØªØ­Ø±ÛŒÚ©Ø´ÙˆÙ†',\n",
              " 'Â«Ø¬Ù„Ø§Ø¯',\n",
              " 'Ø´Ø¯ÛŒÙ†ØŸØŸ',\n",
              " 'Ø±Ù‡Ø§Ù…',\n",
              " 'Ù†Ú¯Ø±Ø§Ù†Ù….',\n",
              " 'Ø¹Ú©Ø³:',\n",
              " 'Ú¯ÛŒÙ†Ø³Ù‡',\n",
              " 'Ø¯ÙˆÙ…ÛŒØ´']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Advantages:**\n",
        "- One-Hot-Encoding has the advantage that the result is binary rather than ordinal and that everything sits in an orthogonal vector space.\n",
        "\n",
        "###**Disadvantages:**\n",
        "- The disadvantage is that for high cardinality, the feature space can really blow up quickly and you start fighting with the curse of dimensionality and the vectors is so sparse.\n",
        "- If we add a document to our corpus, every vectors will change!"
      ],
      "metadata": {
        "id": "Jq8RvuNnsoQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. TF-IDF"
      ],
      "metadata": {
        "id": "qHeSYFUKw5gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. find the TF-IDF of all tweets.\n",
        "# 2. choose one tweets randomly.\n",
        "# 3. find 10 nearest tweets from chosen tweet.\n",
        "\n",
        "n = len(corpus)\n",
        "m = len(distinct_words)\n",
        "\n",
        "word2ind = {}\n",
        "df = {}\n",
        "tf_idf={}\n",
        "words_per_doc = []\n",
        "\n",
        "for i, w in enumerate(distinct_words):\n",
        "  word2ind[w]= i\n",
        "\n",
        "for d in corpus:\n",
        "  for w in d.split(' '):\n",
        "    if w in df:\n",
        "      df[w] += 1\n",
        "    else:\n",
        "      df[w] = 1\n",
        "\n",
        "l = len(word2ind)\n",
        "\n",
        "for d in corpus:\n",
        "  words_per_doc = d.split(' ')\n",
        "  tf_idf[d] = np.zeros(l)\n",
        "  c = Counter(words_per_doc)\n",
        "  word_per_doc_items = list(c.items())\n",
        "  all_words = len(words_per_doc)\n",
        "  for item in word_per_doc_items:\n",
        "    tf = item[1] / all_words\n",
        "    idf = np.log10( n / df[item[0]])  \n",
        "    i = word2ind[item[0]]\n",
        "    tf_idf[d][i] = tf * idf\n",
        "\n",
        "embedding_dict = {}\n",
        "for d in corpus:\n",
        "  embedding_dict[d] = torch.from_numpy(tf_idf[d])\n",
        "\n",
        "t = corpus[217]\n",
        "print(\"Chosen Tweet:\" + t + \"\\n\")\n",
        "neighbors = find_k_nearest_neighbors(t, embedding_dict, k)\n",
        "neighbors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCWg0Vr2SmHB",
        "outputId": "af9ce10c-b5bb-44e9-f998-5f0849c39203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen Tweet:Ø¨Ø±Ø§ÛŒ Ø§Ù‡ÙˆØ§Ø²\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ø¨Ø±Ø§ÛŒ Ø§Ù‡ÙˆØ§Ø²',\n",
              " 'Ø§Ø¯Ø§Ù…Ù‡ Ø§Ø¹ØªØ±Ø§Ø¶Ø§Øª ÙˆØ³Ø· Ø®ÛŒØ§Ø¨Ø§Ù†\\u200cÙ‡Ø§ÛŒ Ø§Ù‡ÙˆØ§Ø² Ø¬Ù…Ø¹Ù‡ Û¸ Ù…Ù‡Ø±ØŒ Ø§Ø¹ØªØ±Ø§Ø¶Ø§Øª Ø³Ø±Ø§Ø³Ø±ÛŒ Ø§Ù‡ÙˆØ§Ø² ',\n",
              " 'Ø¯Ù…ØªÙˆÙ† Ú¯Ø±Ù… Ø¨Ú†Ù‡ Ù‡Ø§ÛŒ Ø§Ù‡ÙˆØ§Ø² Ø§ÛŒÙ† Ø§Ø² Ø§Ù…Ø´Ø¨ ØŒØ§Ù…ÛŒØ¯ÙˆØ§Ø±ÛŒÙ… ÙØ±Ø¯Ø§ Ù‡Ù…Ù‡ Ù…Ø­Ù„Ø§Øª Ø§Ù‡ÙˆØ§Ø² Ø±Ùˆ Ø¯Ø±Ú©Ù Ø®ÛŒØ§Ø¨Ø§Ù† Ø¨Ø¨ÛŒÙ†ÛŒÙ… ',\n",
              " 'Now in Ù…Ø±Ø¯Ù… Ù…Ø¹ØªØ±Ø¶ Ø¯Ø± Ø§Ù‡ÙˆØ§Ø² Ø¨Ù‡ Ø®ÛŒØ§Ø¨Ø§Ù†\\u200cÙ‡Ø§ Ø±ÙØªÙ†Ø¯ ',\n",
              " 'Ø§Ø² Ø·Ø±Ù Ù…Ù‡Ø¯ÛŒ Ø¯Ø±ÛŒØ³ Ù…ÙˆÙ„Ø§ÛŒÛŒ Ø¢Ø¨Ø§Ù† Û¹Û¸ ğŸ–¤ Ø§Ù‡ÙˆØ§Ø²',\n",
              " 'Ù…Ù† ØªØ§ Ø­Ø§Ù„Ø§ Ù†Ù…ÛŒØ¯ÙˆÙ†Ø³ØªÙ… Ø­Ø²Ø¨ÛŒ Ø¨Ù‡ Ù†Ø§Ù… ØªØ¶Ø§Ù…Ù† Ø¯Ù…ÙˆÚ©Ø±Ø§ØªÛŒÚ© Ø§Ù‡ÙˆØ§Ø² (Ø¹Ø±Ø¨Ø³ØªØ§Ù†) Ø¨Ø§ Û¶Û°Û° ÙØ§Ù„ÙˆØ¦Ø± ÙˆØ¬ÙˆØ¯ Ø®Ø§Ø±Ø¬ÛŒ Ø¯Ø§Ø±Ù‡ ÙˆÙ„ÛŒ Ø¨Ù‡ Ø¨Ø±Ú©Øª Ø´Ø¨Ú©Ù‡ Ø§ÛŒØ±Ø§Ù† Ø§ÛŒÙ†ØªØ±Ù†Ø´Ù†Ø§Ù„ Ø¨Ø§Ù‡Ø§Ø´ÙˆÙ† Ø¢Ø´Ù†Ø§ Ø´Ø¯Ù… ',\n",
              " 'Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø§ÛŒ',\n",
              " 'Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø§ÛŒ',\n",
              " 'Ø¨Ø±Ø§ÛŒ',\n",
              " 'Ù…Ù† Ø§Ù‡Ù„ Ø³Ù†Ù†Ø¯Ø¬ Ù‡Ø³ØªÙ… Ù…Ù† Ø§Ù‡Ù„ Ø§Ù‡ÙˆØ§Ø² Ù‡Ø³ØªÙ… Ù…Ù† Ø§Ù‡Ù„ ØªÙ‡Ø±Ø§Ù† Ù‡Ø³ØªÙ… Ù…Ù† Ø§Ù‡Ù„ Ø¹Ø³Ù„ÙˆÛŒÙ‡ Ù‡Ø³ØªÙ… Ù…Ù† Ø§Ù‡Ù„ Ø§Ø´Ù†ÙˆÛŒÙ‡ Ù‡Ø³ØªÙ… Ù…Ù† Ø§ÛŒØ±Ø§Ù†ÛŒÙ… ØŒØ§ÛŒØ±Ø§Ù†ÛŒ']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Advantages:**\n",
        "- Easy to compute\n",
        "- You have some basic metric to extract the most descriptive terms in a document\n",
        "- You can easily compute the similarity between 2 documents using it\n",
        "\n",
        "###**Disadvantages:**\n",
        "- TF-IDF is based on the bag-of-words (BoW) model, therefore it does not capture position in text, semantics, co-occurrences in different documents, etc.\n",
        "- For this reason, TF-IDF is only useful as a lexical level feature.\n",
        "- Cannot capture semantics (e.g. as compared to topic models, word embeddings)\n",
        "- Because tf-idf can experience the curse of dimensionality, it can also experience memory inefficiency. "
      ],
      "metadata": {
        "id": "JD98SPr-AfMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Word2Vec"
      ],
      "metadata": {
        "id": "jIuLL3Mn2sLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. train a word2vec model base on all tweets for each year.\n",
        "# 2. find 10 nearest words from \"Ø²Ù†Ø¯Ú¯ÛŒ\"\n",
        "\n",
        "data = []\n",
        "for d in corpus:\n",
        "  data.append(d.split(' '))\n",
        "\n",
        "model = Word2Vec(data, min_count = 1, size = 100, window = 5)\n",
        "nearest_words = model.wv.most_similar(word)\n",
        "nearest_words"
      ],
      "metadata": {
        "id": "TCnxqaVY2zCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee13246-3f9a-4ee3-ff8c-29659b096af0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Ø²Ù†', 0.994603157043457),\n",
              " ('Ø²Ù†Ø¯Ú¯ÛŒØŒ', 0.993238091468811),\n",
              " ('Ø²Ù†ØŒ', 0.9929890632629395),\n",
              " ('Ø¢Ø²Ø§Ø¯ÛŒ.', 0.9924159646034241),\n",
              " ('Ø¢Ø²Ø§Ø¯ÛŒ', 0.9920729398727417),\n",
              " ('Ù…ÛŒÙ‡Ù†', 0.9915899038314819),\n",
              " ('ØŒØ²Ù†Ø¯Ú¯ÛŒ', 0.991539478302002),\n",
              " ('Ø§Ø²Ø§Ø¯ÛŒ', 0.9906849265098572),\n",
              " ('Ù‡Ø³ØªÛŒÙ….', 0.9897983074188232),\n",
              " ('Ø§Ù…ÛŒØ¯', 0.9890443086624146)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Advantages:**\n",
        "- The idea is very intuitive, which transforms the unlabled raw corpus into labeled data (by mapping the target word to its context word), and learns the representation of words in a classification task.\n",
        "- The data can be fed into the model in an online way and needs little preprocessing, thus requires little memory.\n",
        "- The mapping between the target word to its context word implicitly embeds the sub-linear relationship into the vector space of words, so that relationships like â€œking:man as queen:womanâ€ can be infered by word vectors.\n",
        "\n",
        "###**Disadvantages:**\n",
        "- The sub-linear relationships are not explicitly defined. There is little theoretical support behind such characteristic.\n",
        "- The model could be very difficult to train if use the softmax function, since the number of categories is too large (the size of vocabulary)."
      ],
      "metadata": {
        "id": "x_3i8-hpEIwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Contextualized embedding"
      ],
      "metadata": {
        "id": "RSdlWMl64aPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"HooshvareLab/bert-base-parsbert-uncased\""
      ],
      "metadata": {
        "id": "GfKEqNml6eEB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_text_preparation(text, tokenizer):\n",
        "  marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "  tokenized_text = tokenizer.tokenize(marked_text)\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  segments_ids = [1]*len(indexed_tokens)\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensor = torch.tensor([segments_ids])\n",
        "  return tokenized_text, tokens_tensor, segments_tensor"
      ],
      "metadata": {
        "id": "lBJLAs7mWbn5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embeddings(tokens_tensor, segments_tensor, model):\n",
        "  with torch.no_grad():\n",
        "    outputs = model(tokens_tensor, segments_tensor)\n",
        "    hidden_states = outputs[2]\n",
        "  token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "  token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "  token_embeddings = token_embeddings.permute(1,0,2)\n",
        "  token_vecs_sum = []\n",
        "  for token in token_embeddings:\n",
        "    sum_vec = torch.sum(token[-4:], dim=0)\n",
        "    token_vecs_sum.append(sum_vec)\n",
        "  return token_vecs_sum"
      ],
      "metadata": {
        "id": "35wdTHuNXcdL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. fine tune a bert model base on all tweets for each year.\n",
        "# 2. find 10 nearest words from \"Ø²Ù†Ø¯Ú¯ÛŒ\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "uc8hBCnn4cV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf98f98-b61d-4241-bcc7-54b8377c68f1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at HooshvareLab/bert-base-parsbert-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_embeddings = []\n",
        "context_tokens = []\n",
        "for sentence in corpus[:100]:\n",
        "  tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(sentence, tokenizer)\n",
        "  list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
        "  tokens = OrderedDict()\n",
        "  for token in tokenized_text[1:-1]:\n",
        "    if token in tokens:\n",
        "      tokens[token] += 1\n",
        "    else:\n",
        "      tokens[token] = 1\n",
        "    token_indices = [i for i, t in enumerate(tokenized_text) if t == token]\n",
        "    current_index = token_indices[tokens[token]-1]\n",
        "    token_vec = list_token_embeddings[current_index]\n",
        "    context_tokens.append(token)\n",
        "    context_embeddings.append(token_vec)"
      ],
      "metadata": {
        "id": "zdcpyIIuYKLd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distinct_words = set()\n",
        "for w in context_tokens:\n",
        "  distinct_words.add(w)\n",
        "len(distinct_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yc-sLUnmj2U",
        "outputId": "feb3f412-8622-437d-cc8c-839707f46eea"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "710"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dict = {}\n",
        "for w1 in distinct_words:\n",
        "  sum = 0\n",
        "  c = 0\n",
        "  for i, w2 in enumerate(context_tokens):\n",
        "    if(w1 == w2):\n",
        "      sum += context_embeddings[i]\n",
        "      c += 1\n",
        "  embedding_dict[w1] = sum / c"
      ],
      "metadata": {
        "id": "Bzs7xTXdnRyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_k_nearest_neighbors(word, embedding_dict, k)"
      ],
      "metadata": {
        "id": "0bXcKkhOmFBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c966bc90-9ed1-4fe7-a4ba-861b9d03852b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ø²Ù†Ø¯Ú¯ÛŒ',\n",
              " 'Ø§Ø²Ø§Ø¯ÛŒ',\n",
              " 'Ø²Ù†',\n",
              " 'Ù…Ø¨Ø§Ø±Ø²Ù‡',\n",
              " 'Ø´Ø¬Ø§Ø¹Øª',\n",
              " 'Ø§Ø²Ø¯ÙˆØ§Ø¬',\n",
              " 'Ø§ÛŒØ±Ø§Ù†',\n",
              " 'Ù…Ù‡Ø³Ø§',\n",
              " 'Ø¨Ø±Ø§ÛŒ',\n",
              " 'ÙˆØ·Ù†Ù…']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Advantages:**\n",
        "- BERT works well for task-specific models. The state of the art model, BERT, has been trained on a large corpus, making it easier for smaller, more defined nlp tasks.\n",
        "\n",
        "- Metrics can be fine-tuned and be used immediately.\n",
        "\n",
        "- The accuracy of the model is outstanding because it is frequently updated. You can achieve this with successful fine-tuning training.\n",
        "\n",
        "- The BERT model is available and pre-trained in more than 100 languages. This can be useful for projects that are not English-based.\n",
        "\n",
        "\n",
        "###**Disadvantages:**\n",
        "- The main drawbacks of using BERT and other big neural language models is the computational resources needed to train/fine-tune and make inferences.\n",
        "\n",
        "- Most of the drawbacks of BERT can be linked to its size. While training the data on a large corpus significantly helps how the computer predicts and learns, there is also another side to it. They include:\n",
        "\n",
        "- The model is large because of the training structure and corpus.\n",
        "\n",
        "- It is slow to train because it is big and there are a lot of weights to update.\n",
        "\n",
        "- It is expensive. It requires more computation because of its size, which comes at a cost."
      ],
      "metadata": {
        "id": "XOcb1sL-FwAl"
      }
    }
  ]
}