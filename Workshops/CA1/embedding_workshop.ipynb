{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCb3cVwzrfbb",
        "outputId": "6f9ff9de-9124-498a-bd81-f45661aa039f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_DIR = '/content/gdrive/MyDrive/mahsa_amini_data.zip'"
      ],
      "metadata": {
        "id": "SXT4_E5qePVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/gdrive/MyDrive/mahsa_amini_data.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHQqTYO5e671",
        "outputId": "14a2c992-2850-49d6-8183-ec33eabf7ad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/gdrive/MyDrive/mahsa_amini_data.zip\n",
            "replace mahsa_amini_data.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: mahsa_amini_data.csv    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/mahsa_amini_data.csv'"
      ],
      "metadata": {
        "id": "Z4_5Td-QfNHm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q"
      ],
      "metadata": {
        "id": "ghSKa_EcR693"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "6iSmmQgAipNm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from collections import OrderedDict\n",
        "from datasets import load_dataset\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "HtCgCrUVtU_J"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)"
      ],
      "metadata": {
        "id": "pfDri3MGtFm-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_k_nearest_neighbors(word, embedding_dict, k):\n",
        "  words_cosine_similarity = dict()\n",
        "  for token in embedding_dict.keys():\n",
        "    words_cosine_similarity[token] = cos(embedding_dict[word], embedding_dict[token]).item()\n",
        "  words_cosine_similarity = dict(sorted(words_cosine_similarity.items(), key=lambda item: item[1]))\n",
        "  return list(words_cosine_similarity.keys())[-k:][::-1]\n",
        "\n",
        "def delete_hashtag_usernames(text):\n",
        "  try:\n",
        "    result = []\n",
        "    for word in text.split():\n",
        "      if word[0] not in ['@', '#']:\n",
        "        result.append(word)\n",
        "    return ' '.join(result)\n",
        "  except:\n",
        "    return ''\n",
        "\n",
        "def delete_url(text):\n",
        "  text = re.sub(r'http\\S+', '', text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "UXy2OcR7s84V"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"زندگی\"\n",
        "k = 10"
      ],
      "metadata": {
        "id": "mf-3eSz6udw1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0. Data preprocessing"
      ],
      "metadata": {
        "id": "cz4cWWgA4xBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. extract all tweets from files and save them in memory base on each year.\n",
        "# 2. remove urls, hashtags and usernames.\n",
        "corpus = []\n",
        "my_file = pd.read_csv(path)['Text']"
      ],
      "metadata": {
        "id": "bG5awXKo40HS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in my_file:\n",
        "  temp = delete_hashtag_usernames(line)\n",
        "  corpus.append(delete_url(temp))"
      ],
      "metadata": {
        "id": "nngkD00LjE3i"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. One hot encoding"
      ],
      "metadata": {
        "id": "Nqr5DLDYuKd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. find one hot encoding of each word for each year\n",
        "# 2. find 10 nearest words from \"زندگی\"\n",
        "distinct_words = set()\n",
        "for line in corpus:\n",
        "  for w in line.split(' '):\n",
        "    distinct_words.add(w)\n",
        "\n",
        "l = len(distinct_words)\n",
        "embedding_dict = {}\n",
        "for i, w in enumerate(distinct_words):\n",
        "  v1 = np.zeros(l)\n",
        "  v1[i] = 1\n",
        "  v2 = torch.from_numpy(v1)\n",
        "  embedding_dict[w] = v2\n",
        "\n",
        "neighbors = find_k_nearest_neighbors(word, embedding_dict, k)\n",
        "neighbors"
      ],
      "metadata": {
        "id": "OPqc0I0yuNlI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9be30cff-61a9-429c-92d7-4d0b1688d432"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['آزادی',\n",
              " 'دادین.',\n",
              " 'تحریکشون',\n",
              " '«جلاد',\n",
              " 'شدین؟؟',\n",
              " 'رهام',\n",
              " 'نگرانم.',\n",
              " 'عکس:',\n",
              " 'گینسه',\n",
              " 'دومیش']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Advantages:**\n",
        "- One-Hot-Encoding has the advantage that the result is binary rather than ordinal and that everything sits in an orthogonal vector space.\n",
        "\n",
        "###**Disadvantages:**\n",
        "- The disadvantage is that for high cardinality, the feature space can really blow up quickly and you start fighting with the curse of dimensionality and the vectors is so sparse.\n",
        "- If we add a document to our corpus, every vectors will change!"
      ],
      "metadata": {
        "id": "Jq8RvuNnsoQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. TF-IDF"
      ],
      "metadata": {
        "id": "qHeSYFUKw5gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. find the TF-IDF of all tweets.\n",
        "# 2. choose one tweets randomly.\n",
        "# 3. find 10 nearest tweets from chosen tweet.\n",
        "\n",
        "n = len(corpus)\n",
        "m = len(distinct_words)\n",
        "\n",
        "word2ind = {}\n",
        "df = {}\n",
        "tf_idf={}\n",
        "words_per_doc = []\n",
        "\n",
        "for i, w in enumerate(distinct_words):\n",
        "  word2ind[w]= i\n",
        "\n",
        "for d in corpus:\n",
        "  for w in d.split(' '):\n",
        "    if w in df:\n",
        "      df[w] += 1\n",
        "    else:\n",
        "      df[w] = 1\n",
        "\n",
        "l = len(word2ind)\n",
        "\n",
        "for d in corpus:\n",
        "  words_per_doc = d.split(' ')\n",
        "  tf_idf[d] = np.zeros(l)\n",
        "  c = Counter(words_per_doc)\n",
        "  word_per_doc_items = list(c.items())\n",
        "  all_words = len(words_per_doc)\n",
        "  for item in word_per_doc_items:\n",
        "    tf = item[1] / all_words\n",
        "    idf = np.log10( n / df[item[0]])  \n",
        "    i = word2ind[item[0]]\n",
        "    tf_idf[d][i] = tf * idf\n",
        "\n",
        "embedding_dict = {}\n",
        "for d in corpus:\n",
        "  embedding_dict[d] = torch.from_numpy(tf_idf[d])\n",
        "\n",
        "t = corpus[217]\n",
        "print(\"Chosen Tweet:\" + t + \"\\n\")\n",
        "neighbors = find_k_nearest_neighbors(t, embedding_dict, k)\n",
        "neighbors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCWg0Vr2SmHB",
        "outputId": "af9ce10c-b5bb-44e9-f998-5f0849c39203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen Tweet:برای اهواز\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['برای اهواز',\n",
              " 'ادامه اعتراضات وسط خیابان\\u200cهای اهواز جمعه ۸ مهر، اعتراضات سراسری اهواز ',\n",
              " 'دمتون گرم بچه های اهواز این از امشب ،امیدواریم فردا همه محلات اهواز رو درکف خیابان ببینیم ',\n",
              " 'Now in مردم معترض در اهواز به خیابان\\u200cها رفتند ',\n",
              " 'از طرف مهدی دریس مولایی آبان ۹۸ 🖤 اهواز',\n",
              " 'من تا حالا نمیدونستم حزبی به نام تضامن دموکراتیک اهواز (عربستان) با ۶۰۰ فالوئر وجود خارجی داره ولی به برکت شبکه ایران اینترنشنال باهاشون آشنا شدم ',\n",
              " 'برای برای',\n",
              " 'برای برای برای',\n",
              " 'برای',\n",
              " 'من اهل سنندج هستم من اهل اهواز هستم من اهل تهران هستم من اهل عسلویه هستم من اهل اشنویه هستم من ایرانیم ،ایرانی']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Advantages:**\n",
        "- Easy to compute\n",
        "- You have some basic metric to extract the most descriptive terms in a document\n",
        "- You can easily compute the similarity between 2 documents using it\n",
        "\n",
        "###**Disadvantages:**\n",
        "- TF-IDF is based on the bag-of-words (BoW) model, therefore it does not capture position in text, semantics, co-occurrences in different documents, etc.\n",
        "- For this reason, TF-IDF is only useful as a lexical level feature.\n",
        "- Cannot capture semantics (e.g. as compared to topic models, word embeddings)\n",
        "- Because tf-idf can experience the curse of dimensionality, it can also experience memory inefficiency. "
      ],
      "metadata": {
        "id": "JD98SPr-AfMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Word2Vec"
      ],
      "metadata": {
        "id": "jIuLL3Mn2sLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. train a word2vec model base on all tweets for each year.\n",
        "# 2. find 10 nearest words from \"زندگی\"\n",
        "\n",
        "data = []\n",
        "for d in corpus:\n",
        "  data.append(d.split(' '))\n",
        "\n",
        "model = Word2Vec(data, min_count = 1, size = 100, window = 5)\n",
        "nearest_words = model.wv.most_similar(word)\n",
        "nearest_words"
      ],
      "metadata": {
        "id": "TCnxqaVY2zCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee13246-3f9a-4ee3-ff8c-29659b096af0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('زن', 0.994603157043457),\n",
              " ('زندگی،', 0.993238091468811),\n",
              " ('زن،', 0.9929890632629395),\n",
              " ('آزادی.', 0.9924159646034241),\n",
              " ('آزادی', 0.9920729398727417),\n",
              " ('میهن', 0.9915899038314819),\n",
              " ('،زندگی', 0.991539478302002),\n",
              " ('ازادی', 0.9906849265098572),\n",
              " ('هستیم.', 0.9897983074188232),\n",
              " ('امید', 0.9890443086624146)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Advantages:**\n",
        "- The idea is very intuitive, which transforms the unlabled raw corpus into labeled data (by mapping the target word to its context word), and learns the representation of words in a classification task.\n",
        "- The data can be fed into the model in an online way and needs little preprocessing, thus requires little memory.\n",
        "- The mapping between the target word to its context word implicitly embeds the sub-linear relationship into the vector space of words, so that relationships like “king:man as queen:woman” can be infered by word vectors.\n",
        "\n",
        "###**Disadvantages:**\n",
        "- The sub-linear relationships are not explicitly defined. There is little theoretical support behind such characteristic.\n",
        "- The model could be very difficult to train if use the softmax function, since the number of categories is too large (the size of vocabulary)."
      ],
      "metadata": {
        "id": "x_3i8-hpEIwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Contextualized embedding"
      ],
      "metadata": {
        "id": "RSdlWMl64aPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"HooshvareLab/bert-base-parsbert-uncased\""
      ],
      "metadata": {
        "id": "GfKEqNml6eEB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_text_preparation(text, tokenizer):\n",
        "  marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "  tokenized_text = tokenizer.tokenize(marked_text)\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  segments_ids = [1]*len(indexed_tokens)\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensor = torch.tensor([segments_ids])\n",
        "  return tokenized_text, tokens_tensor, segments_tensor"
      ],
      "metadata": {
        "id": "lBJLAs7mWbn5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embeddings(tokens_tensor, segments_tensor, model):\n",
        "  with torch.no_grad():\n",
        "    outputs = model(tokens_tensor, segments_tensor)\n",
        "    hidden_states = outputs[2]\n",
        "  token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "  token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "  token_embeddings = token_embeddings.permute(1,0,2)\n",
        "  token_vecs_sum = []\n",
        "  for token in token_embeddings:\n",
        "    sum_vec = torch.sum(token[-4:], dim=0)\n",
        "    token_vecs_sum.append(sum_vec)\n",
        "  return token_vecs_sum"
      ],
      "metadata": {
        "id": "35wdTHuNXcdL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. fine tune a bert model base on all tweets for each year.\n",
        "# 2. find 10 nearest words from \"زندگی\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "uc8hBCnn4cV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf98f98-b61d-4241-bcc7-54b8377c68f1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at HooshvareLab/bert-base-parsbert-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_embeddings = []\n",
        "context_tokens = []\n",
        "for sentence in corpus[:100]:\n",
        "  tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(sentence, tokenizer)\n",
        "  list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
        "  tokens = OrderedDict()\n",
        "  for token in tokenized_text[1:-1]:\n",
        "    if token in tokens:\n",
        "      tokens[token] += 1\n",
        "    else:\n",
        "      tokens[token] = 1\n",
        "    token_indices = [i for i, t in enumerate(tokenized_text) if t == token]\n",
        "    current_index = token_indices[tokens[token]-1]\n",
        "    token_vec = list_token_embeddings[current_index]\n",
        "    context_tokens.append(token)\n",
        "    context_embeddings.append(token_vec)"
      ],
      "metadata": {
        "id": "zdcpyIIuYKLd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distinct_words = set()\n",
        "for w in context_tokens:\n",
        "  distinct_words.add(w)\n",
        "len(distinct_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yc-sLUnmj2U",
        "outputId": "feb3f412-8622-437d-cc8c-839707f46eea"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "710"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dict = {}\n",
        "for w1 in distinct_words:\n",
        "  sum = 0\n",
        "  c = 0\n",
        "  for i, w2 in enumerate(context_tokens):\n",
        "    if(w1 == w2):\n",
        "      sum += context_embeddings[i]\n",
        "      c += 1\n",
        "  embedding_dict[w1] = sum / c"
      ],
      "metadata": {
        "id": "Bzs7xTXdnRyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_k_nearest_neighbors(word, embedding_dict, k)"
      ],
      "metadata": {
        "id": "0bXcKkhOmFBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c966bc90-9ed1-4fe7-a4ba-861b9d03852b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['زندگی',\n",
              " 'ازادی',\n",
              " 'زن',\n",
              " 'مبارزه',\n",
              " 'شجاعت',\n",
              " 'ازدواج',\n",
              " 'ایران',\n",
              " 'مهسا',\n",
              " 'برای',\n",
              " 'وطنم']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Advantages:**\n",
        "- BERT works well for task-specific models. The state of the art model, BERT, has been trained on a large corpus, making it easier for smaller, more defined nlp tasks.\n",
        "\n",
        "- Metrics can be fine-tuned and be used immediately.\n",
        "\n",
        "- The accuracy of the model is outstanding because it is frequently updated. You can achieve this with successful fine-tuning training.\n",
        "\n",
        "- The BERT model is available and pre-trained in more than 100 languages. This can be useful for projects that are not English-based.\n",
        "\n",
        "\n",
        "###**Disadvantages:**\n",
        "- The main drawbacks of using BERT and other big neural language models is the computational resources needed to train/fine-tune and make inferences.\n",
        "\n",
        "- Most of the drawbacks of BERT can be linked to its size. While training the data on a large corpus significantly helps how the computer predicts and learns, there is also another side to it. They include:\n",
        "\n",
        "- The model is large because of the training structure and corpus.\n",
        "\n",
        "- It is slow to train because it is big and there are a lot of weights to update.\n",
        "\n",
        "- It is expensive. It requires more computation because of its size, which comes at a cost."
      ],
      "metadata": {
        "id": "XOcb1sL-FwAl"
      }
    }
  ]
}